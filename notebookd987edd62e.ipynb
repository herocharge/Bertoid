{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"36c9ba63d6754e7fb5749e13d3b1c739":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b18a922e95b47fc8e02387b7c150795","IPY_MODEL_dfb646e5bfb74b169f6f0639d8b768bd","IPY_MODEL_80a447fadf6f47e1b0669b113f3f103d"],"layout":"IPY_MODEL_827424cc69ab408bb0c7d06e15c3a2b2"}},"7b18a922e95b47fc8e02387b7c150795":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0ac7341a84149bfac034a4137dd6116","placeholder":"​","style":"IPY_MODEL_7d66a660a56b40baad2cc2ebb90c4e7e","value":"Loading checkpoint shards: 100%"}},"dfb646e5bfb74b169f6f0639d8b768bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8ba833750bd42ada98629b0101cfef7","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_084b0a1e3f734cc7b640e2d567c40435","value":2}},"80a447fadf6f47e1b0669b113f3f103d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2371ac498914405853221e0792cc8ec","placeholder":"​","style":"IPY_MODEL_47872914a3bc413f9172432a3850cd45","value":" 2/2 [01:00&lt;00:00, 27.54s/it]"}},"827424cc69ab408bb0c7d06e15c3a2b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ac7341a84149bfac034a4137dd6116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d66a660a56b40baad2cc2ebb90c4e7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8ba833750bd42ada98629b0101cfef7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"084b0a1e3f734cc7b640e2d567c40435":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2371ac498914405853221e0792cc8ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47872914a3bc413f9172432a3850cd45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TbL8zCvkro8","outputId":"6fd31330-c7e7-487f-f33b-d03ddbae18cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"dzmOPlOZ-Qtp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code from https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model name\nnew_model = \"llama-2-7b-miniguanaco\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 1\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 1\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"id":"U1wStldN_jNR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\n# dataset = load_dataset(dataset_name, split=\"train\")\n# type(dataset)","metadata":{"id":"0l3qSJ1yAEFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2Zd_RhXFAUvy","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"e38ec21d-3484-4413-e5f5-18d4f2bc5b37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UtO2IddPzSDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('/kaggle/input/mashqa-train/train_webmd_squad_v2_full.json', 'r') as file:\n    d = json.load(file)\n\nQUERY_SEPARATOR = \"<QSEP>\"\nANS_START = \"<ANSSTART>\"\nANS_END = \"<ANSEND>\"\nEMPTY = \"<EMPTY>\"\ndata = d['data']\ninputs = []\noutputs = []\nmx_len = 0\nMAX_LEN = 2000\nfor dt in data:\n    for paragraph in dt['paragraphs']:\n        for qa in paragraph['qas']:\n            assert(len(qa['answers']) == 1) # passed for train_full\n            # input.append(qa['question'] + QUERY_SEPARATOR + paragraph['context'])\n            inp = qa['question'] + QUERY_SEPARATOR\n            ans = \"\"\n            for idx, sent in enumerate(paragraph['sent_list']):\n                inp += sent + \" \"\n                if idx in qa['answers'][0]['answer_span']:\n                    ans += sent + \" \"\n                if len(inp) >= MAX_LEN:\n                    inputs.append(inp)\n                    if ans == \"\":\n                      ans = EMPTY\n                    outputs.append(ans)\n                    mx_len = max(mx_len, len(outputs[-1]))\n                    mx_len = max(mx_len, len(inputs[-1]))\n                    inp = qa['question'] + QUERY_SEPARATOR\n                    ans = \"\"\n            if inp != qa['question'] + QUERY_SEPARATOR:\n                inputs.append(inp)\n                if ans == \"\":\n                  ans = EMPTY\n                outputs.append(ans)\n                mx_len = max(mx_len, len(outputs[-1]))\n                mx_len = max(mx_len, len(inputs[-1]))\n                inp = qa['question'] + QUERY_SEPARATOR\n                ans = \"\"\nmx_len","metadata":{"id":"RTkoD2XSlaWV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d82e349c-b079-4897-875e-6104633b9a2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs[:10]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"AviUFR7wzki3","outputId":"585140c2-61e7-4ea2-b1ae-e78766cd99ea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers.data import Dataset\nfrom datasets import Dataset\n\nchanged = True\n","metadata":{"id":"O8FWW_bDeBjG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = []\nfor x, y in zip(inputs, outputs):\n    train_dataset.append(\"<s>[INST]<<SYS>><</SYS>>\" +x+\"[/INST]\"+y+\" </s>\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\nprint(type(dataset))\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nif changed:\n    QUERY_SEPARATOR = \"<QSEP>\"\n    ANS_START = \"<ANSSTART>\"\n    ANS_END = \"<ANSEND>\"\n    EMPTY = \"<EMPTY>\"\n    tokenizer.add_tokens(QUERY_SEPARATOR)\n    #   tokenizer.add_tokens(ANS_START)\n    #   tokenizer.add_tokens(ANS_END)\n    tokenizer.add_tokens(EMPTY)\n#     # Tokenize the texts\n#     tokenized_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n\n#     # Tokenize and split the outputs\n#     tokenized_outputs = tokenizer(outputs, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n    #   dataset = train_dataset\n    dataset = Dataset.from_dict({\"text\": train_dataset[:200]})\n\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["36c9ba63d6754e7fb5749e13d3b1c739","7b18a922e95b47fc8e02387b7c150795","dfb646e5bfb74b169f6f0639d8b768bd","80a447fadf6f47e1b0669b113f3f103d","827424cc69ab408bb0c7d06e15c3a2b2","e0ac7341a84149bfac034a4137dd6116","7d66a660a56b40baad2cc2ebb90c4e7e","f8ba833750bd42ada98629b0101cfef7","084b0a1e3f734cc7b640e2d567c40435","b2371ac498914405853221e0792cc8ec","47872914a3bc413f9172432a3850cd45"]},"id":"TPSRQpbW_kUR","outputId":"a754d14c-0c3f-44f1-f6cb-415c018f98e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"id":"4P4BvqABKfSE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"<<SYS>><</SYS>> \\n\\n Gastritis is an inflammation, irritation, or erosion of the lining of the stomach. It can occur suddenly (acute) or gradually (chronic). Gastritis can be caused by irritation due to excessive alcohol use, chronic vomiting, stress, or the use of certain medications such as aspirin or other anti-inflammatory drugs. It may also be caused by any of the following: Helicobacter pylori ( H. pylori): A bacteria that lives in the mucous lining of the stomach; without treatment, the infection can lead to ulcers, and in some people, stomach cancer.  What are the symptoms of gastritis?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScTAqTppKlUs","outputId":"db8776f4-3f6b-44c0-f418-7194e1328e6c","trusted":true},"execution_count":null,"outputs":[]}]}